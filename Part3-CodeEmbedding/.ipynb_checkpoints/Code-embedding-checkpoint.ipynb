{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08d5bab",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cff9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pyparsing\n",
    "import jsonpickle\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval as make_tuple\n",
    "#\n",
    "#import base64\n",
    "import pybase64 as base64\n",
    "#\n",
    "import torch\n",
    "import einops\n",
    "from einops import rearrange\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#\n",
    "from datasets import Dataset\n",
    "#\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "#\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d85d7",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a8fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de8452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILE = \"files_of_interest.json\"\n",
    "FILE = \"files_of_interest_8.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc52a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_EMBEDDING_SIZE = 768\n",
    "BERT_MAX_TOKENS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6b22e",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgrammingLanguage(Enum):\n",
    "    PYTHON = \"Python\"\n",
    "    JAVA = \"Java\"\n",
    "    JAVASCRIPT = \"JavaScript\"\n",
    "    GO = \"Go\"\n",
    "    PHP = \"PHP\"\n",
    "    RUBY = \"Ruby\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_comment_filters(programming_language):\n",
    "        if programming_language in [ProgrammingLanguage.PYTHON, ProgrammingLanguage.RUBY]:\n",
    "            return [pyparsing.pythonStyleComment.suppress()]\n",
    "        if programming_language in [ProgrammingLanguage.JAVA, ProgrammingLanguage.JAVASCRIPT, ProgrammingLanguage.GO]:\n",
    "            return [pyparsing.cppStyleComment.suppress()]\n",
    "        return [pyparsing.pythonStyleComment.suppress(), pyparsing.cppStyleComment.suppress()]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_lang_from_file_name(file_name):\n",
    "        if file_name.endswith(\".py\"):\n",
    "            return ProgrammingLanguage.PYTHON\n",
    "        if file_name.endswith(\".php\"):\n",
    "            return ProgrammingLanguage.PHP\n",
    "        if file_name.endswith(\".js\"):\n",
    "            return ProgrammingLanguage.JAVASCRIPT\n",
    "        if file_name.endswith(\".java\"):\n",
    "            return ProgrammingLanguage.JAVA\n",
    "        if file_name.endswith(\".go\"):\n",
    "            return ProgrammingLanguage.GO\n",
    "        return ProgrammingLanguage.RUBY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1c690",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, FILE)) as f_in:\n",
    "    for line in f_in:\n",
    "        data = jsonpickle.decode(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fac040",
   "metadata": {},
   "source": [
    "## Decode files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_states = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in data:\n",
    "    print(repo)\n",
    "    for fileId, file_name in enumerate(data[repo]):\n",
    "        if (fileId+1)%10 == 0:\n",
    "            print(f\"\\t{fileId+1}/{len(data[repo])}\")\n",
    "        for commit in data[repo][file_name]:\n",
    "            file_content = None\n",
    "            file_encoding = None\n",
    "            for file in commit[\"files\"]:\n",
    "                if file_name == file[\"name\"]:\n",
    "                    file_sha = file[\"sha\"]\n",
    "                    file_content = file[\"content\"]\n",
    "                    if \"content_encoding\" in file:\n",
    "                        file_encoding = file[\"content_encoding\"]\n",
    "                    break\n",
    "            if file_content and file_encoding and file_encoding==\"base64\":\n",
    "                try:\n",
    "                    decoded_content = base64.b64decode(file_content).decode(\"utf-8\")\n",
    "                    clean_decoded_content = decoded_content\n",
    "                    programming_lang = ProgrammingLanguage.get_lang_from_file_name(file_name)\n",
    "                    for commentFilter in ProgrammingLanguage.get_comment_filters(programming_lang):\n",
    "                        clean_decoded_content = commentFilter.transformString(clean_decoded_content)\n",
    "                    clean_decoded_content = \"\\n\".join([s for s in clean_decoded_content.split(\"\\n\") if len(s.strip()) > 0])\n",
    "                    file_states[(repo, file_sha, commit[\"sha\"])] = {\n",
    "                        \"source\": clean_decoded_content\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(\"ERROR: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3843ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = [repo for repo in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_file_data = jsonpickle.encode(file_states)\n",
    "#\n",
    "with open(os.path.join(DATA_FOLDER, \"files_of_interest_source_lookup.json\"), \"w\") as f_out:\n",
    "    f_out.write(encoded_file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf47c34-897d-4573-94d2-dda8d7530002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, \"repo_to_id.json\"), \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        REPO_TO_ID = jsonpickle.decode(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d121d2-d447-4f87-91e8-4ffbe415a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_TO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c5ed21-ea00-4aca-8379-da862a9e2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_TO_ID[repos[0]] = '8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc245116-2de9-4539-b4e1-be4d6eed6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_TO_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576979c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Alternative - load previous save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, \"file_of_interest_embedding_lookup.json\"), \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        file_states = jsonpickle.decode(line)\n",
    "#       \n",
    "adjusted_file_states = {}\n",
    "for entry in file_states:\n",
    "    t = make_tuple(entry)\n",
    "    adjusted_file_states[t] = file_states[entry]\n",
    "#\n",
    "file_states = adjusted_file_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = list(set([repo for repo, _, _ in file_states]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd99f35b",
   "metadata": {},
   "source": [
    "## Alternative - Load existing file source lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, \"files_of_interest_source_lookup.json\"), \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        file_states = jsonpickle.decode(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9cd426",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_file_states = {}\n",
    "for entry in file_states:\n",
    "    t = make_tuple(entry)\n",
    "    adjusted_file_states[t] = file_states[entry]\n",
    "#\n",
    "file_states = adjusted_file_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = list(set([repo for repo, _, _ in file_states]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in repos:\n",
    "    cnt = 0\n",
    "    for entry_repo, _, _ in file_states:\n",
    "        if repo == entry_repo:\n",
    "            cnt = cnt + 1\n",
    "    print(f\"{repo} \\t => {cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6444aac",
   "metadata": {},
   "source": [
    "## Encode files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc188386",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_dataset = {}\n",
    "for repo in repos:\n",
    "    nlp_dataset[repo] = []\n",
    "    for key in file_states:\n",
    "        file_repo, _, _  = key\n",
    "        source = file_states[key][\"source\"]\n",
    "        if repo == file_repo:\n",
    "            nlp_dataset[repo].append({\n",
    "                \"text\": source,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in nlp_dataset:\n",
    "    print(f\"{repo} \\t => {len(nlp_dataset[repo])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ca5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/graphcodebert-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd366ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de552b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 256\n",
    "size = 512\n",
    "batch_size = 64\n",
    "#\n",
    "for repo in REPO_TO_ID:\n",
    "    if repo not in nlp_dataset:\n",
    "        continue\n",
    "    print(repo)\n",
    "    for i, instance in enumerate(nlp_dataset[repo]):\n",
    "        if (i+1)%10==0:\n",
    "            print(f\"\\t => {i+1}/{len(nlp_dataset[repo])}\")\n",
    "        source = instance[\"text\"]\n",
    "        #===\n",
    "        if \"embedding\" in instance and instance[\"embedding\"] is not None:\n",
    "            continue\n",
    "        #===\n",
    "        if source is None or len(source.strip()) == 0:\n",
    "            instance[\"embedding\"] = None\n",
    "            continue            \n",
    "        #===\n",
    "        tokens = tokenizer.tokenize(source)\n",
    "        #===\n",
    "        chunks = []\n",
    "        for j in range(0, len(tokens), step):\n",
    "            chunk = [tokenizer.cls_token] + tokens[j:min(len(tokens),j+size)-1]\n",
    "            chunks.append(chunk)\n",
    "            if j+size>len(tokens):\n",
    "                break\n",
    "        #===\n",
    "        if len(chunks) == 0:\n",
    "            instance[\"embedding\"] = None\n",
    "            continue\n",
    "        #===\n",
    "        while len(chunks[-1]) < size:\n",
    "            chunks[-1].append(tokenizer.pad_token)\n",
    "        #===\n",
    "        for j in range(len(chunks)):\n",
    "            chunks[j] = tokenizer.convert_tokens_to_ids(chunks[j])\n",
    "        #===\n",
    "        all_hidden = []\n",
    "        for j in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[j:min(j+batch_size, len(chunks))]\n",
    "            #===\n",
    "            t = torch.tensor(batch).to(device)\n",
    "            #===\n",
    "            with torch.no_grad():\n",
    "                hs = model(t, output_hidden_states=True)\n",
    "                last_hiddens = hs.hidden_states[-1][:,0,:]\n",
    "                all_hidden.append(last_hiddens.detach().cpu())\n",
    "        #===\n",
    "        all_hidden = torch.cat(all_hidden, 0)\n",
    "        instance[\"embedding\"] = torch.mean(all_hidden, axis=0).numpy() \n",
    "        #\n",
    "        del t\n",
    "        torch.cuda.empty_cache()\n",
    "    #==========================================\n",
    "    #Save the created data\n",
    "    save_data = {}\n",
    "    save_data[repo] = nlp_dataset[repo]\n",
    "    encoded = jsonpickle.encode(save_data)\n",
    "    with open(os.path.join(DATA_FOLDER, f\"file_of_interest_embedding_lookup_{REPO_TO_ID[repo]}.json\"), \"w\") as f_out:\n",
    "        f_out.write(encoded)\n",
    "    #del nlp_dataset[repo]\n",
    "    #==========================================\n",
    "encoded = jsonpickle.encode(nlp_dataset)\n",
    "with open(os.path.join(DATA_FOLDER, f\"file_of_interest_embedding_lookup.json\"), \"w\") as f_out:\n",
    "    f_out.write(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f541bf8-2549-45ae-9bd7-7f832129d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d1530",
   "metadata": {},
   "source": [
    "## Order encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_file_embeddings = {}\n",
    "for repo in data:\n",
    "    sorted_file_embeddings[repo] = {}\n",
    "    for file_name in data[repo]:\n",
    "        sorted_embedding = []\n",
    "        sorted_commits = sorted(data[repo][file_name], key=lambda c: c[\"date\"])\n",
    "        for commit in sorted_commits:\n",
    "            this_file = None\n",
    "            for file in commit[\"files\"]:\n",
    "                if file_name == file[\"name\"]:\n",
    "                    this_file = file\n",
    "                    break            \n",
    "            #\n",
    "            if this_file:\n",
    "                embedding = file_states[(repo, file[\"sha\"], commit[\"sha\"])][\"embedding\"]\n",
    "                sorted_embedding.append({\"commit\": commit, \"embedding\": embedding})\n",
    "        sorted_file_embeddings[repo][file_name] = sorted_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sorted_file_embeddings = jsonpickle.encode(sorted_file_embeddings)\n",
    "#\n",
    "with open(os.path.join(DATA_FOLDER, f\"p2_file_of_interest_sorted_embeddings.json\"), \"w\") as f_out:\n",
    "    f_out.write(encoded_sorted_file_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcac21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
