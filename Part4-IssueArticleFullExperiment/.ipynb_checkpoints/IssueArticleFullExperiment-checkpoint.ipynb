{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7edd7b4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc816a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jsonpickle\n",
    "from ast import literal_eval as make_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ddb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "#\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec8ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "#\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "#\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd6d8f-8ece-43b4-8dbc-6ce5983af0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4097ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd332c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bbcdf",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"./../data/\"\n",
    "#\n",
    "FILE_OF_INTEREST = \"files_of_interest.json\"\n",
    "FILE_OF_INTEREST_SOURCE = \"files_of_interest_source_lookup.json\"\n",
    "LABELED_FILE_KEY = \"labeled_issues_of_interest_\"\n",
    "EMBEDDING_FILE_KEY = \"file_of_interest_embedding_lookup_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49aca8",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file(commit, file_name):\n",
    "    for file in commit[\"files\"]:\n",
    "        if file[\"name\"] == file_name:\n",
    "            return file\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25907f4a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d67ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files of interest \n",
    "with open(os.path.join(DATA_FOLDER, FILE_OF_INTEREST), \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        file_of_interest_data = jsonpickle.decode(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_TO_ID = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled issues\n",
    "labeled_issues_of_interest_data = {}\n",
    "for file in os.listdir(DATA_FOLDER):\n",
    "    if LABELED_FILE_KEY in file:\n",
    "        repoId = file.replace(LABELED_FILE_KEY, \"\").replace(\".json\", \"\")\n",
    "        with open(os.path.join(DATA_FOLDER, file), \"r\") as f_in:\n",
    "            for line in f_in:\n",
    "                repo_labeled_issues = jsonpickle.decode(line)\n",
    "        for repo in repo_labeled_issues:\n",
    "            REPO_TO_ID[repo] = repoId\n",
    "        labeled_issues_of_interest_data.update(repo_labeled_issues)\n",
    "#===\n",
    "for repo in labeled_issues_of_interest_data:\n",
    "    if \"list\" in str(type(labeled_issues_of_interest_data[repo])):\n",
    "        adjusted_labeled_issues_of_interest = {}\n",
    "        for issue in labeled_issues_of_interest_data[repo]:\n",
    "            if issue is not None:\n",
    "                adjusted_labeled_issues_of_interest[str(issue[\"number\"])] = issue\n",
    "        labeled_issues_of_interest_data[repo] = adjusted_labeled_issues_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02847a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files of interest source\n",
    "with open(os.path.join(DATA_FOLDER, FILE_OF_INTEREST_SOURCE), \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        file_states = jsonpickle.decode(line)\n",
    "#\n",
    "adjusted_file_states = {}\n",
    "for entry in file_states:\n",
    "    t = make_tuple(entry)\n",
    "    adjusted_file_states[t] = file_states[entry]\n",
    "#\n",
    "file_states = adjusted_file_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file embedding \n",
    "file_embeddings_data = {}\n",
    "for file in os.listdir(DATA_FOLDER):\n",
    "    if EMBEDDING_FILE_KEY in file:\n",
    "        with open(os.path.join(DATA_FOLDER, file), \"r\") as f_in:\n",
    "            for line in f_in:\n",
    "                repo_file_embeddings = jsonpickle.decode(line)   \n",
    "        file_embeddings_data.update(repo_file_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b32b90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Adjust embeddings - missing file and commit id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5917a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjuted_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in file_states:\n",
    "    if \"embedding\" in file_states[key]:\n",
    "        continue\n",
    "    #\n",
    "    repo, _, _ = key\n",
    "    #\n",
    "    for entry in file_embeddings_data[repo]:\n",
    "        if file_states[key][\"source\"] == entry[\"text\"] and \"embedding\" in entry:\n",
    "            file_states[key][\"embedding\"] = entry[\"embedding\"]\n",
    "            adjuted_cnt = adjuted_cnt + 1\n",
    "    #\n",
    "    if (adjuted_cnt+1)%50==0:\n",
    "        print(adjuted_cnt)\n",
    "print(adjuted_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = jsonpickle.encode(file_states)\n",
    "with open(os.path.join(DATA_FOLDER, \"files_of_interest_with_embeddings.json\"), \"w\") as f_out:\n",
    "    f_out.write(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce017669",
   "metadata": {},
   "source": [
    "## Load adjusted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f7596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_OF_INTEREST_WITH_EMBEDDINGS = \"files_of_interest_with_embeddings.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d5ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files of interest with embeddings\n",
    "with open(os.path.join(DATA_FOLDER, FILE_OF_INTEREST_WITH_EMBEDDINGS), \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        file_states = jsonpickle.decode(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_file_states = {}\n",
    "for entry in file_states:\n",
    "    t = make_tuple(entry)\n",
    "    adjusted_file_states[t] = file_states[entry]\n",
    "#\n",
    "file_states = adjusted_file_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81532eed-964f-4d4c-b5b1-33ab7bb31df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693764b-48b7-43f6-8448-a8c09e516e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, \"repo_to_id.json\"), \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        REPO_TO_ID = jsonpickle.decode(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebee5d1",
   "metadata": {},
   "source": [
    "## Construct datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc1e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "for repo in file_of_interest_data:\n",
    "    if repo not in labeled_issues_of_interest_data:\n",
    "        continue\n",
    "    dataset[repo] = {}\n",
    "    for file_name in file_of_interest_data[repo]:\n",
    "        commits = file_of_interest_data[repo][file_name]\n",
    "        commits = sorted(commits, key=lambda c: c[\"date\"])\n",
    "        #\n",
    "        all_refs_cnt = 0\n",
    "        for commit in commits:\n",
    "            file = find_file(commit, file_name)\n",
    "            if file is None:\n",
    "                continue\n",
    "            all_refs_cnt = all_refs_cnt + len(commit[\"refs\"])        \n",
    "        #\n",
    "        for commit in commits:\n",
    "            file = find_file(commit, file_name)\n",
    "            if file is None:\n",
    "                continue\n",
    "            #\n",
    "            key = (repo, file[\"sha\"], commit[\"sha\"])\n",
    "            has_source = key in file_states and \"source\" in file_states[key]\n",
    "            has_embedding = key in file_states and \"embedding\" in file_states[key] and file_states[key][\"embedding\"] is not None \n",
    "            if not has_source or not has_embedding:\n",
    "                continue\n",
    "            #\n",
    "            bug_cnt = 0\n",
    "            undefined_cnt = 0\n",
    "            for ref in commit[\"refs\"]:\n",
    "                if ref in labeled_issues_of_interest_data[repo] and labeled_issues_of_interest_data[repo][ref]:\n",
    "                    if \"type\" in labeled_issues_of_interest_data[repo][ref]:\n",
    "                        if labeled_issues_of_interest_data[repo][ref][\"type\"] == \"Bug\":\n",
    "                            bug_cnt = bug_cnt + 1\n",
    "                    else:\n",
    "                        undefined_cnt = undefined_cnt + 1\n",
    "                else:\n",
    "                        undefined_cnt = undefined_cnt + 1\n",
    "            #\n",
    "            if has_source and has_embedding and (bug_cnt>0 or undefined_cnt==0):\n",
    "                if file_name not in dataset[repo]:\n",
    "                    dataset[repo][file_name] = []\n",
    "                #\n",
    "                source = file_states[key][\"source\"]\n",
    "                lines_of_code = len([line for line in source.split(\"\\n\") if len(line.strip()) > 0 ])\n",
    "                embedding = file_states[key][\"embedding\"]\n",
    "                bug = 1 if bug_cnt > 0 else 0\n",
    "                #\n",
    "                dataset[repo][file_name].append((source, lines_of_code, embedding, len(commits), all_refs_cnt, len(commit[\"refs\"]), commit[\"refs\"], bug))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f42d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data = []\n",
    "for repo in dataset:\n",
    "    cnt = 0\n",
    "    bug_cnt = 0\n",
    "    for file in dataset[repo]:\n",
    "        cnt = cnt + 1\n",
    "        for version in dataset[repo][file]:\n",
    "            source, lines_of_code, embedding, commit_cnt, all_refs_cnt, refs_cnt, refs, bug = version\n",
    "            #\n",
    "            if bug > 0:\n",
    "                bug_cnt = bug_cnt + 1\n",
    "                break\n",
    "            #\n",
    "        r = bug_cnt/cnt\n",
    "        random_f1 = 2*r/(r+1)\n",
    "    print_data.append([repo, bug_cnt, cnt, f\"{round(100*bug_cnt/cnt, 2)}%\", f\"{round(100*random_f1, 2)}%\"])\n",
    "print(tabulate(print_data, headers=[\"Repo\", \"BugCnt\", \"Cnt\", \"Share\", \"MaxF1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461efba7",
   "metadata": {},
   "source": [
    "## Classification experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prop_from_version(prop, version):\n",
    "    source, lines_of_code, embedding, commit_cnt, all_refs_cnt, refs_cnt, refs, bug = version\n",
    "    if prop == \"source\":\n",
    "        return source\n",
    "    if prop == \"loc\":\n",
    "        return lines_of_code\n",
    "    if prop == \"embedding\":\n",
    "        return embedding\n",
    "    if prop == \"commit_cnt\":\n",
    "        return commit_cnt\n",
    "    if prop == \"all_refs_cnt\":\n",
    "        return all_refs_cnt\n",
    "    if prop == \"refs_cnt\":\n",
    "        return refs_cnt\n",
    "    if prop == \"refs\":\n",
    "        return refs\n",
    "    if prop == \"bug\":\n",
    "        return bug\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_diff_vec(embeddings):\n",
    "    if len(embeddings) < 2:\n",
    "        return np.zeros(embeddings[0].shape)\n",
    "    else:\n",
    "        diffs = []\n",
    "        for i in range(len(embeddings)-1):\n",
    "            diff = embeddings[i] - embeddings[i+1]\n",
    "            diffs.append(diff)\n",
    "        return np.asarray(sum(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_x(versions):\n",
    "    locs = [get_prop_from_version(\"loc\", v) for v in versions]\n",
    "    avg_loc = np.asarray([sum(locs)/len(locs)])\n",
    "    #\n",
    "    embeddings = [get_prop_from_version(\"embedding\", v) for v in versions]\n",
    "    x_mean = np.asarray(sum(embeddings) / len(versions))\n",
    "    x_diff = calc_diff_vec(embeddings)\n",
    "    #\n",
    "    commit_cnt = np.asarray([get_prop_from_version(\"commit_cnt\", versions[0])])\n",
    "    #\n",
    "    all_refs_cnt = np.asarray([get_prop_from_version(\"all_refs_cnt\", versions[0])])\n",
    "    #\n",
    "    ref_cnts = [get_prop_from_version(\"refs_cnt\", v) for v in versions]\n",
    "    avg_ref_cnt = np.asarray([sum(ref_cnts) / len(ref_cnts)])\n",
    "    #\n",
    "    x = np.concatenate((avg_loc, x_mean, x_diff, commit_cnt, all_refs_cnt, avg_ref_cnt))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_y(versions):\n",
    "    bug_cnt = sum([get_prop_from_version(\"bug\", v) for v in versions])\n",
    "    return 1 if bug_cnt > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcacd6-6e25-4607-831a-0fd1d6651081",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [(\"LogisticRegression\", lambda: LogisticRegression()), \n",
    "          (\"KNeighborsClassifier\", lambda: KNeighborsClassifier(1)), \n",
    "          (\"GaussianNB\", lambda: GaussianNB()), \n",
    "          (\"DecisionTreeClassifier\", lambda: DecisionTreeClassifier()),\n",
    "          #(\"SVM\", lambda: SVC())\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6c26b-fdb9-4947-89c1-016aec71465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"Model\", \"Precision\", \"Recall\", \"F1\", \"MCC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6d93c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86903296",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_experimental_results = {}\n",
    "for repo in dataset:\n",
    "    gold_experimental_results[repo] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REP_CNT = 30\n",
    "for repo in dataset:\n",
    "    print(repo)\n",
    "    #======\n",
    "    X = []\n",
    "    Y = []\n",
    "    for file in dataset[repo]:\n",
    "        x = calc_x(dataset[repo][file])\n",
    "        y = calc_y(dataset[repo][file])\n",
    "        #\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(Y)\n",
    "    #======\n",
    "    for rep in range(REP_CNT):\n",
    "        if (rep+1) % 5 == 0:\n",
    "            print(f\"\\t {rep+1}/{REP_CNT}\")\n",
    "        #==\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        #\n",
    "        for model_name, model_provider in MODELS:\n",
    "            classifier = model_provider()\n",
    "            #\n",
    "            classifier.fit(X_train, y_train)\n",
    "            #\n",
    "            yp = classifier.predict(X_test)\n",
    "            #\n",
    "            classifier_precision = precision_score(y_test, yp)\n",
    "            classifier_recall = recall_score(y_test, yp)\n",
    "            classifier_f1 = f1_score(y_test, yp)\n",
    "            classifier_mcc = matthews_corrcoef(y_test, yp)\n",
    "            #\n",
    "            gold_experimental_results[repo].append([model_name, classifier_precision, classifier_recall, classifier_f1, classifier_mcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in gold_experimental_results:\n",
    "    df = pd.DataFrame(gold_experimental_results[repo], columns=columns)\n",
    "    print(repo)\n",
    "    print(df.groupby([\"Model\"]).mean())\n",
    "    #boxplot = df.boxplot(column=columns) \n",
    "    #plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7728f",
   "metadata": {},
   "source": [
    "### Experiment heursitic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748679e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_datasets = {}\n",
    "for repo in dataset:\n",
    "    if repo not in labeled_issues_of_interest_data:\n",
    "        continue\n",
    "    #\n",
    "    nlp_datasets[repo] = []\n",
    "    #\n",
    "    for ref in labeled_issues_of_interest_data[repo]:\n",
    "        issue = labeled_issues_of_interest_data[repo][ref]\n",
    "        if issue is None or \"type\" not in issue or issue[\"type\"] is None or \"text\" not in issue:\n",
    "            continue\n",
    "        label = 1 if issue[\"type\"] == 'Bug' else 0\n",
    "        nlp_datasets[repo].append({\"text\": issue[\"text\"], \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic_model_results = {}\n",
    "for repo in nlp_datasets:\n",
    "    p = []\n",
    "    y = []\n",
    "    for issue in nlp_datasets[repo]:\n",
    "        is_bug = False\n",
    "        for key in [\"bug\", \"fix\"]:\n",
    "            if key in issue[\"text\"]:\n",
    "                is_bug = True\n",
    "                break\n",
    "        p.append(1 if is_bug else 0)\n",
    "        y.append(issue[\"label\"])\n",
    "        #\n",
    "        classifier_precision = precision_score(y, p)\n",
    "        classifier_recall = recall_score(y, p)    \n",
    "        classifier_f1 = f1_score(y, p)\n",
    "        classifier_mcc = matthews_corrcoef(y, p)\n",
    "        #\n",
    "        heuristic_model_results[repo] = [classifier_precision, classifier_recall, classifier_f1, classifier_mcc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8bd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_y_keyword_heuristic(versions, repo, keywords=[\"bug\", \"fix\"], locations=[\"title\", \"body\"]):\n",
    "    for version in versions:\n",
    "        refs = get_prop_from_version(\"refs\", version)\n",
    "        for ref in refs:\n",
    "            if ref in labeled_issues_of_interest_data[repo] and labeled_issues_of_interest_data[repo][ref]:\n",
    "                issue = labeled_issues_of_interest_data[repo][ref]\n",
    "                for word in keywords:\n",
    "                    for location in locations:\n",
    "                        if issue[location] is not None:\n",
    "                            if word in issue[location].lower():\n",
    "                                return 1\n",
    "    return 0            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic_experimental_results = {}\n",
    "for repo in dataset:\n",
    "    heuristic_experimental_results[repo] = []\n",
    "#\n",
    "heuristics_labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "REP_CNT = 30\n",
    "for repo in dataset:\n",
    "    print(repo)\n",
    "    #======\n",
    "    X = []\n",
    "    YT = []\n",
    "    YH = []\n",
    "    for file in dataset[repo]:\n",
    "        versions = dataset[repo][file]\n",
    "        x = calc_x(versions)\n",
    "        yt = calc_y(versions)\n",
    "        yh = calc_y_keyword_heuristic(versions, repo)\n",
    "        #\n",
    "        X.append(x)\n",
    "        YT.append(yt)\n",
    "        YH.append(yh)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(YT)\n",
    "    yh = np.asarray(YH)\n",
    "    #======\n",
    "    heuristics_labels[repo] = {\n",
    "        \"Y\": YT,\n",
    "        \"YE\": YH\n",
    "    }\n",
    "    #======\n",
    "    for rep in range(REP_CNT):\n",
    "        if (rep+1) % 5 == 0:\n",
    "            print(f\"\\t {rep+1}/{REP_CNT}\")\n",
    "        #==\n",
    "        X_train, X_test, y_train, _, _, y_test  = train_test_split(X, yh, y, test_size=0.2)\n",
    "        #\n",
    "        for model_name, model_provider in MODELS:\n",
    "            classifier = model_provider()\n",
    "            #\n",
    "            classifier.fit(X_train, y_train)\n",
    "            #\n",
    "            yp = classifier.predict(X_test)\n",
    "            #\n",
    "            classifier_precision = precision_score(y_test, yp)\n",
    "            classifier_recall = recall_score(y_test, yp)\n",
    "            classifier_f1 = f1_score(y_test, yp)\n",
    "            classifier_mcc = matthews_corrcoef(y_test, yp)\n",
    "            #\n",
    "            heuristic_experimental_results[repo].append([model_name, classifier_precision, classifier_recall, classifier_f1, classifier_mcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in heuristic_experimental_results:\n",
    "    df = pd.DataFrame(heuristic_experimental_results[repo], columns=columns)\n",
    "    print(repo)\n",
    "    print(df.groupby([\"Model\"]).mean())\n",
    "    #boxplot = df.boxplot(column=columns) \n",
    "    #plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2d58c",
   "metadata": {},
   "source": [
    "### Experiment improved Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in labeled_issues_of_interest_data:\n",
    "    for ref in labeled_issues_of_interest_data[repo]:\n",
    "        issue = labeled_issues_of_interest_data[repo][ref]\n",
    "        if issue is None or \"type\" not in issue or issue[\"type\"] is None:\n",
    "            continue\n",
    "        issue_title = issue[\"title\"]\n",
    "        issue_description = issue[\"body\"]\n",
    "        issue_title = \"\" if issue_title is None else issue_title\n",
    "        issue_description = \"\" if issue_description is None else issue_description\n",
    "        #\n",
    "        text = (issue_title + \" \" + issue_description).lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"  \", \" \")\n",
    "        text = ''.join([c for c in text  if c.isalpha()])\n",
    "        issue[\"text\"] = text\n",
    "        #\n",
    "        issue[\"description\"] = issue_description.lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"  \", \" \")\n",
    "        issue[\"description\"] = ''.join([c for c in issue[\"description\"]  if c.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_datasets = {}\n",
    "for repo in dataset:\n",
    "    if repo not in labeled_issues_of_interest_data:\n",
    "        continue\n",
    "    #\n",
    "    nlp_datasets[repo] = []\n",
    "    #\n",
    "    for ref in labeled_issues_of_interest_data[repo]:\n",
    "        issue = labeled_issues_of_interest_data[repo][ref]\n",
    "        if issue is None or \"type\" not in issue or issue[\"type\"] is None or \"text\" not in issue:\n",
    "            continue\n",
    "        label = 1 if issue[\"type\"] == 'Bug' else 0\n",
    "        nlp_datasets[repo].append({\"text\": issue[\"text\"], \"title\": issue[\"title\"].lower(), \"description\": issue[\"description\"], \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_words = ['bug', 'fix', 'issue', 'out', 'error',\n",
    "                  'not', 'line', 'test']\n",
    "#\n",
    "combination_cnt = 0x1 << len(starting_words)\n",
    "strategies = []\n",
    "for i in range(1, combination_cnt):\n",
    "    keywords = []\n",
    "    for w_id in range(len(starting_words)):\n",
    "        if ((0x1 << w_id) & i) > 0:\n",
    "            keywords.append(starting_words[w_id])\n",
    "    strategies.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_strategy_for_repo = {}\n",
    "for repo in nlp_datasets:\n",
    "    best_strategy = None\n",
    "    best_strategy_f1 = 0\n",
    "    #best_fcnt = 50000\n",
    "    for description_keywords in strategies:\n",
    "        for title_keywords in strategies:\n",
    "            p = []\n",
    "            y = []\n",
    "            for issue in nlp_datasets[repo]:\n",
    "                is_bug = False\n",
    "                for key in title_keywords:\n",
    "                    if key in issue[\"title\"]:\n",
    "                        is_bug = True\n",
    "                        break\n",
    "                if is_bug is False:\n",
    "                    for key in description_keywords:\n",
    "                        if key in issue[\"description\"]:\n",
    "                            is_bug = True\n",
    "                            break\n",
    "                p.append(1 if is_bug else 0)\n",
    "                y.append(issue[\"label\"])\n",
    "        #\n",
    "        f1 = f1_score(y, p)\n",
    "        if f1>best_strategy_f1:\n",
    "            best_strategy_f1 = f1\n",
    "            best_strategy = (description_keywords, title_keywords)\n",
    "            print(\"============================\")\n",
    "            print(f\"{repo} => BEST: {best_strategy}\")\n",
    "            print(\"============================\")         \n",
    "    best_strategy_for_repo[repo] = best_strategy\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe48f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_heuristic_model_results = {}\n",
    "for repo in nlp_datasets:\n",
    "    description_keywords, title_keywords = best_strategy_for_repo[repo]\n",
    "    p = []\n",
    "    y = []\n",
    "    for issue in nlp_datasets[repo]:\n",
    "        is_bug = False\n",
    "        for key in title_keywords:\n",
    "            if key in issue[\"title\"]:\n",
    "                is_bug = True\n",
    "                break\n",
    "        if is_bug is False:\n",
    "            for key in description_keywords:\n",
    "                if key in issue[\"description\"]:\n",
    "                    is_bug = True\n",
    "                    break\n",
    "        p.append(1 if is_bug else 0)\n",
    "        y.append(issue[\"label\"])\n",
    "        #\n",
    "        classifier_precision = precision_score(y, p)\n",
    "        classifier_recall = recall_score(y, p)    \n",
    "        classifier_f1 = f1_score(y, p)\n",
    "        classifier_mcc = matthews_corrcoef(y, p)\n",
    "        #\n",
    "        improved_heuristic_model_results[repo] = [classifier_precision, classifier_recall, classifier_f1, classifier_mcc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa47bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_y_improved_keyword_heuristic(versions, repo, keywords):\n",
    "    description_keywords, title_keywords = keywords\n",
    "    for version in versions:\n",
    "        refs = get_prop_from_version(\"refs\", version)\n",
    "        for ref in refs:\n",
    "            if ref in labeled_issues_of_interest_data[repo] and labeled_issues_of_interest_data[repo][ref]:\n",
    "                issue = labeled_issues_of_interest_data[repo][ref]\n",
    "                #\n",
    "                for word in title_keywords:\n",
    "                    if issue[\"title\"] is not None:\n",
    "                        if word in issue[\"title\"].lower():\n",
    "                            return 1                \n",
    "                #                         \n",
    "                for word in description_keywords:\n",
    "                    if issue[\"body\"] is not None:\n",
    "                        text = issue[\"body\"].lower().replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"  \", \" \")\n",
    "                        text = ''.join([c for c in text  if c.isalpha()])\n",
    "                        if word in text:\n",
    "                            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_heuristic_experimental_results = {}\n",
    "for repo in dataset:\n",
    "    improved_heuristic_experimental_results[repo] = []\n",
    "#\n",
    "improved_heuristics_labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "REP_CNT = 30\n",
    "for repo in dataset:\n",
    "    print(repo)\n",
    "    #======\n",
    "    X = []\n",
    "    YT = []\n",
    "    YH = []\n",
    "    for file in dataset[repo]:\n",
    "        versions = dataset[repo][file]\n",
    "        x = calc_x(versions)\n",
    "        yt = calc_y(versions)\n",
    "        yh = calc_y_improved_keyword_heuristic(versions, repo, best_strategy_for_repo[repo])\n",
    "        #\n",
    "        X.append(x)\n",
    "        YT.append(yt)\n",
    "        YH.append(yh)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(YT)\n",
    "    yh = np.asarray(YH)\n",
    "    #======\n",
    "    improved_heuristics_labels[repo] = {\n",
    "        \"Y\": YT,\n",
    "        \"YE\": YH\n",
    "    }\n",
    "    #======\n",
    "    for rep in range(REP_CNT):\n",
    "        if (rep+1) % 5 == 0:\n",
    "            print(f\"\\t {rep+1}/{REP_CNT}\")\n",
    "        #==\n",
    "        X_train, X_test, y_train, _, _, y_test  = train_test_split(X, yh, y, test_size=0.2)\n",
    "        #\n",
    "        for model_name, model_provider in MODELS:\n",
    "            classifier = model_provider()\n",
    "            #\n",
    "            classifier.fit(X_train, y_train)\n",
    "            #\n",
    "            yp = classifier.predict(X_test)\n",
    "            #\n",
    "            classifier_precision = precision_score(y_test, yp)\n",
    "            classifier_recall = recall_score(y_test, yp)\n",
    "            classifier_f1 = f1_score(y_test, yp)\n",
    "            classifier_mcc = matthews_corrcoef(y_test, yp)\n",
    "            #\n",
    "            improved_heuristic_experimental_results[repo].append([model_name, classifier_precision, classifier_recall, classifier_f1, classifier_mcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635049d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in improved_heuristic_experimental_results:\n",
    "    df = pd.DataFrame(improved_heuristic_experimental_results[repo], columns=columns)\n",
    "    print(repo)\n",
    "    print(df.groupby([\"Model\"]).mean())\n",
    "    #boxplot = df.boxplot(column=columns) \n",
    "    #plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5249bee",
   "metadata": {},
   "source": [
    "### Experiment NLP - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a68da-b1a2-41e6-9cbf-850db2fb76d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_y_bert(versions, lookup):\n",
    "    for version in versions:\n",
    "        refs = get_prop_from_version(\"refs\", version)\n",
    "        for ref in refs:\n",
    "            ref = int(ref)\n",
    "            if ref in lookup:\n",
    "                if lookup[ref] > 0:\n",
    "                    return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in labeled_issues_of_interest_data:\n",
    "    for ref in labeled_issues_of_interest_data[repo]:\n",
    "        issue = labeled_issues_of_interest_data[repo][ref]\n",
    "        if issue is None or \"type\" not in issue or issue[\"type\"] is None:\n",
    "            continue\n",
    "        issue_title = issue[\"title\"]\n",
    "        issue_description = issue[\"body\"]\n",
    "        issue_title = \"\" if issue_title is None else issue_title\n",
    "        issue_description = \"\" if issue_description is None else issue_description\n",
    "        #\n",
    "        CLEANR = re.compile('<.*?>') \n",
    "        text = re.sub(CLEANR, ' ', issue_description)\n",
    "        text = issue_title + \" \" + text\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = re.sub(' http.*? ', ' [link] ', text)\n",
    "        #\n",
    "        issue[\"text\"] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data = []\n",
    "nlp_datasets = {}\n",
    "for repo in dataset:\n",
    "    if repo not in labeled_issues_of_interest_data:\n",
    "        continue\n",
    "    #\n",
    "    nlp_datasets[repo] = {\n",
    "        \"train\": [],\n",
    "        \"test\": []\n",
    "    }\n",
    "    #\n",
    "    train_bug_cnt = 0\n",
    "    for other_repo in dataset:\n",
    "        if repo == other_repo or other_repo not in labeled_issues_of_interest_data:\n",
    "            continue\n",
    "        for ref in labeled_issues_of_interest_data[other_repo]:\n",
    "            issue = labeled_issues_of_interest_data[other_repo][ref]\n",
    "            if issue is None or \"type\" not in issue or issue[\"type\"] is None or \"text\" not in issue:\n",
    "                continue\n",
    "            label = 1 if issue[\"type\"] == 'Bug' else 0\n",
    "            train_bug_cnt = train_bug_cnt + label\n",
    "            nlp_datasets[repo][\"train\"].append({\"text\": issue[\"text\"], \"label\": label})\n",
    "    #\n",
    "    test_bug_cnt = 0\n",
    "    for ref in labeled_issues_of_interest_data[repo]:\n",
    "        issue = labeled_issues_of_interest_data[repo][ref]\n",
    "        if issue is None or \"type\" not in issue or issue[\"type\"] is None or \"text\" not in issue:\n",
    "            continue\n",
    "        label = 1 if issue[\"type\"] == 'Bug' else 0\n",
    "        test_bug_cnt = test_bug_cnt  + label\n",
    "        nlp_datasets[repo][\"test\"].append({\"issueId\": issue[\"number\"], \"text\": issue[\"text\"], \"label\": label})\n",
    "    #\n",
    "    print_data.append([repo, train_bug_cnt, len(nlp_datasets[repo][\"train\"]), test_bug_cnt, len(nlp_datasets[repo][\"test\"])])\n",
    "print(tabulate(print_data, headers=[\"Repo\", \"TrainBugCnt\", \"TrainCnt\", \"TestBugCnt\", \"TestCnt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f537e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(entry):\n",
    "    return tokenizer(entry[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe09105",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\")\n",
    "#\n",
    "def compute_metrics(eval_pred):\n",
    "    o, y = eval_pred\n",
    "    yp = np.argmax(o, axis=-1)\n",
    "    #\n",
    "    return metric.compute(predictions=yp, references=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c5b95-8903-4de2-aff2-71580a26c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_experimental_results = {}\n",
    "for repo in dataset:\n",
    "    bert_experimental_results[repo] = []\n",
    "#\n",
    "bert_model_results = {}\n",
    "bert_labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4671c029-b0c2-44b7-84b6-5ed9610acb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b1673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "REP_CNT = 30\n",
    "for repo in dataset:\n",
    "    print(repo)\n",
    "    #===============================\n",
    "    # Train BERT\n",
    "    train_nlp = Dataset.from_pandas(pd.DataFrame(nlp_datasets[repo][\"train\"]))\n",
    "    #\n",
    "    permutation = torch.randperm(len(train_nlp)).tolist()\n",
    "    train_cnt = int(len(train_nlp) * TRAIN_SIZE)\n",
    "    train_indices = permutation[:train_cnt]\n",
    "    val_indices = permutation[train_cnt:]\n",
    "    val_nlp = train_nlp.select(val_indices)\n",
    "    train_nlp = train_nlp.select(train_indices)\n",
    "    #\n",
    "    train_nlp = train_nlp.map(tokenize, batched=True)\n",
    "    val_nlp = val_nlp.map(tokenize, batched=True)\n",
    "    #\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "    if device == \"cuda\":\n",
    "        model.cuda()\n",
    "    #\n",
    "    training_args = TrainingArguments(\n",
    "                         overwrite_output_dir=True,\n",
    "                         output_dir=f\"roberta-issue-classifier-{REPO_TO_ID[repo]}\",\n",
    "                         evaluation_strategy=\"epoch\",\n",
    "                         learning_rate=2e-5,\n",
    "                         logging_strategy='epoch',\n",
    "                         per_device_train_batch_size=4,\n",
    "                         per_device_eval_batch_size=4,\n",
    "                         save_total_limit=3,\n",
    "                         num_train_epochs=6, \n",
    "                         gradient_accumulation_steps=4,\n",
    "                         gradient_checkpointing=True,\n",
    "                         weight_decay=1e-3,\n",
    "                         save_strategy='epoch',\n",
    "                         load_best_model_at_end=True)\n",
    "    #\n",
    "    trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_nlp,\n",
    "                eval_dataset=val_nlp,\n",
    "                compute_metrics=compute_metrics)\n",
    "    #\n",
    "    trainer.train()\n",
    "    #\n",
    "    model.save_pretrained(f\"roberta-issue-classifier-{REPO_TO_ID[repo]}/evaluated_model\")\n",
    "    #\n",
    "    test_nlp = Dataset.from_pandas(pd.DataFrame(nlp_datasets[repo][\"test\"]))\n",
    "    test_nlp = test_nlp.map(tokenize, batched=True)\n",
    "    #\n",
    "    copy_test_nlp = test_nlp.select([i for i in range(len(test_nlp))])\n",
    "    copy_test_nlp = copy_test_nlp.remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n",
    "    copy_test_nlp.set_format(\"torch\")\n",
    "    #\n",
    "    del train_nlp\n",
    "    del val_nlp\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    #\n",
    "    eval_dataloader = DataLoader(copy_test_nlp, batch_size=16)\n",
    "    #\n",
    "    model.eval()\n",
    "    #\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_issueId = []\n",
    "    for batch in eval_dataloader:\n",
    "        all_labels.append(batch['labels'].detach().cpu())\n",
    "        all_issueId.append(batch['issueId'].detach().cpu())\n",
    "        del batch[\"issueId\"]\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            hs = model(**batch, output_hidden_states=True)\n",
    "            logits = hs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            last_hiddens = hs.hidden_states[-1][:,0,:]\n",
    "            all_preds.append(predictions)\n",
    "    #\n",
    "    all_labels = torch.cat(all_labels, 0).detach().cpu().numpy()\n",
    "    all_preds = torch.cat(all_preds, 0).detach().cpu().numpy()                          \n",
    "    all_issueId = torch.cat(all_issueId, 0).detach().cpu().numpy()\n",
    "    #\n",
    "    bert_precision = precision_score(all_labels, all_preds)\n",
    "    bert_recall = recall_score(all_labels, all_preds)\n",
    "    bert_f1 = f1_score(all_labels, all_preds)\n",
    "    bert_mcc = matthews_corrcoef(all_labels, all_preds)   \n",
    "    #\n",
    "    bert_model_results[repo] = [bert_precision, bert_recall, bert_f1, bert_mcc]\n",
    "    #\n",
    "    issue_lbl_lookup = {}\n",
    "    for i in range(len(test_nlp)):\n",
    "        issue_bert_lbl = all_preds[i]\n",
    "        issue_id = all_issueId[i]\n",
    "        issue_lbl_lookup[issue_id] = issue_bert_lbl\n",
    "    #===============================\n",
    "    X = []\n",
    "    YT = []\n",
    "    YH = []\n",
    "    for file in dataset[repo]:\n",
    "        versions = dataset[repo][file]\n",
    "        x = calc_x(versions)\n",
    "        yt = calc_y(versions)\n",
    "        yh = calc_y_bert(versions, issue_lbl_lookup)\n",
    "        #\n",
    "        X.append(x)\n",
    "        YT.append(yt)\n",
    "        YH.append(yh)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(YT)\n",
    "    yh = np.asarray(YH)\n",
    "    #======\n",
    "    bert_labels[repo] = {\n",
    "        \"Y\": YT,\n",
    "        \"YE\": YH\n",
    "    }\n",
    "    #======\n",
    "    for rep in range(REP_CNT):\n",
    "        if (rep+1) % 5 == 0:\n",
    "            print(f\"\\t {rep+1}/{REP_CNT}\")\n",
    "        #==\n",
    "        X_train, X_test, y_train, _, _, y_test  = train_test_split(X, yh, y, test_size=0.2)\n",
    "        #\n",
    "        for model_name, model_provider in MODELS:\n",
    "            classifier = model_provider()\n",
    "            #\n",
    "            classifier.fit(X_train, y_train)\n",
    "            #\n",
    "            yp = classifier.predict(X_test)\n",
    "            #\n",
    "            classifier_precision = precision_score(y_test, yp)\n",
    "            classifier_recall = recall_score(y_test, yp)\n",
    "            classifier_f1 = f1_score(y_test, yp)\n",
    "            classifier_mcc = matthews_corrcoef(y_test, yp)\n",
    "            #\n",
    "            bert_experimental_results[repo].append([model_name, classifier_precision, classifier_recall, classifier_f1, classifier_mcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db806b-b96a-468d-b391-5af601f56993",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data = []\n",
    "for repo in bert_model_results:\n",
    "    print_data.append([repo]+bert_model_results[repo])\n",
    "print(tabulate(print_data, headers=[\"Repo\", \"Precision\", \"Recall\", \"F1\", \"MCC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600154b5-ff0c-45ab-9416-e21942e19678",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in bert_experimental_results:\n",
    "    df = pd.DataFrame(bert_experimental_results[repo], columns=columns)\n",
    "    print(repo)\n",
    "    print(df.groupby([\"Model\"]).mean())\n",
    "    #boxplot = df.boxplot(column=columns) \n",
    "    #plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d98342-c0c1-49c8-a22b-9a5deeec2cc5",
   "metadata": {},
   "source": [
    "### Experiment NLP - Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617319d-1370-4f75-888c-ab74d006e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_y_fasttext(versions, lookup):\n",
    "    for version in versions:\n",
    "        refs = get_prop_from_version(\"refs\", version)\n",
    "        for ref in refs:\n",
    "            ref = int(ref)\n",
    "            if ref in lookup:\n",
    "                if lookup[ref] > 0:\n",
    "                    return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c97a2-c9d4-4ce9-b32e-2334704abd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in labeled_issues_of_interest_data:\n",
    "    for ref in labeled_issues_of_interest_data[repo]:\n",
    "        issue = labeled_issues_of_interest_data[repo][ref]\n",
    "        if issue is None or \"type\" not in issue or issue[\"type\"] is None:\n",
    "            continue\n",
    "        issue_title = issue[\"title\"]\n",
    "        issue_description = issue[\"body\"]\n",
    "        issue_title = \"\" if issue_title is None else issue_title\n",
    "        issue_description = \"\" if issue_description is None else issue_description\n",
    "        #\n",
    "        CLEANR = re.compile('<.*?>') \n",
    "        text = re.sub(CLEANR, ' ', issue_description)\n",
    "        text = issue_title + \" \" + text\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = re.sub(' http.*? ', ' [link] ', text)\n",
    "        #\n",
    "        issue[\"text\"] = text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9747175d-069e-46cc-a31c-30f59182c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data = []\n",
    "nlp_datasets = {}\n",
    "for repo in dataset:\n",
    "    if repo not in labeled_issues_of_interest_data:\n",
    "        continue\n",
    "    #\n",
    "    nlp_datasets[repo] = {\n",
    "        \"train\": [],\n",
    "        \"test\": []\n",
    "    }\n",
    "    #\n",
    "    train_bug_cnt = 0\n",
    "    for other_repo in dataset:\n",
    "        if repo == other_repo or other_repo not in labeled_issues_of_interest_data:\n",
    "            continue\n",
    "        for ref in labeled_issues_of_interest_data[other_repo]:\n",
    "            issue = labeled_issues_of_interest_data[other_repo][ref]\n",
    "            if issue is None or \"type\" not in issue or issue[\"type\"] is None or \"text\" not in issue:\n",
    "                continue\n",
    "            label = 1 if issue[\"type\"] == 'Bug' else 0\n",
    "            train_bug_cnt = train_bug_cnt + label\n",
    "            nlp_datasets[repo][\"train\"].append({\"text\": issue[\"text\"], \"label\": label})\n",
    "    #\n",
    "    test_bug_cnt = 0\n",
    "    for ref in labeled_issues_of_interest_data[repo]:\n",
    "        issue = labeled_issues_of_interest_data[repo][ref]\n",
    "        if issue is None or \"type\" not in issue or issue[\"type\"] is None or \"text\" not in issue:\n",
    "            continue\n",
    "        label = 1 if issue[\"type\"] == 'Bug' else 0\n",
    "        test_bug_cnt = test_bug_cnt  + label\n",
    "        nlp_datasets[repo][\"test\"].append({\"issueId\": issue[\"number\"], \"text\": issue[\"text\"], \"label\": label})\n",
    "    #\n",
    "    print_data.append([repo, train_bug_cnt, len(nlp_datasets[repo][\"train\"]), test_bug_cnt, len(nlp_datasets[repo][\"test\"])])\n",
    "print(tabulate(print_data, headers=[\"Repo\", \"TrainBugCnt\", \"TrainCnt\", \"TestBugCnt\", \"TestCnt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e874013b-d10a-4cb2-8c6a-f2faa92663ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_adjust_text(text):\n",
    "    text = \" \".join([line for line in text.split(\"\\n\") if len(line.strip())>0])\n",
    "    text = text.lower()\n",
    "    text = ''.join([c for c in text if c.isalnum() or c.isspace()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b81c3-6026-4949-94ee-fab918336190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasttext_file(filename, data):\n",
    "    with open(filename, \"w\") as f_out:\n",
    "        for entry in data:\n",
    "            text = fasttext_adjust_text(entry[\"text\"])\n",
    "            label = entry[\"label\"]\n",
    "            f_out.write(f\"__label__{label} {text}\") \n",
    "            f_out.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a534f-0f68-471a-bff1-d0f0ec71af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_experimental_results = {}\n",
    "for repo in dataset:\n",
    "    fasttext_experimental_results[repo] = []\n",
    "#\n",
    "fasttext_model_results = {}\n",
    "fasttext_labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0b066-02a6-4b65-9cd4-4906a863ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "REP_CNT = 30\n",
    "for repo in dataset:\n",
    "    print(repo)\n",
    "    #===============================\n",
    "    # Train FastText\n",
    "    train_nlp = nlp_datasets[repo][\"train\"]\n",
    "    train_nlp, val_nlp  = train_test_split(train_nlp, test_size=0.2)\n",
    "    #\n",
    "    write_fasttext_file(\"data.train\", train_nlp)\n",
    "    write_fasttext_file(\"data.valid\", val_nlp)          \n",
    "    #\n",
    "    model = fasttext.train_supervised(input='data.train', autotuneValidationFile='data.valid', autotuneDuration=15*60)\n",
    "    #\n",
    "    model.save_model(f\"fasttext-issue-classifier-{REPO_TO_ID[repo]}.bin\")\n",
    "    #\n",
    "    test_nlp = nlp_datasets[repo][\"test\"]\n",
    "    #\n",
    "    all_issueId = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for entry in test_nlp:\n",
    "        text =  fasttext_adjust_text(entry[\"text\"])\n",
    "        p = model.predict(text)\n",
    "        if len(p[0])>0:\n",
    "            p = 0 if \"0\" in p[0][0] else 1\n",
    "            #\n",
    "            all_issueId.append(entry[\"issueId\"])        \n",
    "            all_labels.append(entry[\"label\"])\n",
    "            all_preds.append(p)\n",
    "    #\n",
    "    fasttext_precision = precision_score(all_labels, all_preds)\n",
    "    fasttext_recall = recall_score(all_labels, all_preds)\n",
    "    fasttext_f1 = f1_score(all_labels, all_preds)\n",
    "    fasttext_mcc = matthews_corrcoef(all_labels, all_preds)   \n",
    "    #\n",
    "    fasttext_model_results[repo] = [fasttext_precision, fasttext_recall, fasttext_f1, fasttext_mcc]\n",
    "    #\n",
    "    issue_lbl_lookup = {}\n",
    "    for i in range(len(all_preds)):\n",
    "        issue_bert_lbl = all_preds[i]\n",
    "        issue_id = all_issueId[i]\n",
    "        issue_lbl_lookup[issue_id] = issue_bert_lbl\n",
    "    #===============================\n",
    "    X = []\n",
    "    YT = []\n",
    "    YH = []\n",
    "    for file in dataset[repo]:\n",
    "        versions = dataset[repo][file]\n",
    "        x = calc_x(versions)\n",
    "        yt = calc_y(versions)\n",
    "        yh = calc_y_fasttext(versions, issue_lbl_lookup)\n",
    "        #\n",
    "        X.append(x)\n",
    "        YT.append(yt)\n",
    "        YH.append(yh)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(YT)\n",
    "    yh = np.asarray(YH)\n",
    "    #======\n",
    "    fasttext_labels[repo] = {\n",
    "        \"Y\": YT,\n",
    "        \"YE\": YH\n",
    "    }\n",
    "    #======\n",
    "    for rep in range(REP_CNT):\n",
    "        if (rep+1) % 5 == 0:\n",
    "            print(f\"\\t {rep+1}/{REP_CNT}\")\n",
    "        #==\n",
    "        X_train, X_test, y_train, _, _, y_test  = train_test_split(X, yh, y, test_size=0.2)\n",
    "        #\n",
    "        for model_name, model_provider in MODELS:\n",
    "            classifier = model_provider()\n",
    "            #\n",
    "            classifier.fit(X_train, y_train)\n",
    "            #\n",
    "            yp = classifier.predict(X_test)\n",
    "            #\n",
    "            classifier_precision = precision_score(y_test, yp)\n",
    "            classifier_recall = recall_score(y_test, yp)\n",
    "            classifier_f1 = f1_score(y_test, yp)\n",
    "            classifier_mcc = matthews_corrcoef(y_test, yp)\n",
    "            #\n",
    "            fasttext_experimental_results[repo].append([model_name, classifier_precision, classifier_recall, classifier_f1, classifier_mcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83177724-4483-472f-9fe9-5583da30dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data = []\n",
    "for repo in fasttext_model_results:\n",
    "    print_data.append([repo]+fasttext_model_results[repo])\n",
    "print(tabulate(print_data, headers=[\"Repo\", \"Precision\", \"Recall\", \"F1\", \"MCC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ab7b3-5dc1-4a58-8b84-5af9926ee7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in fasttext_experimental_results:\n",
    "    df = pd.DataFrame(fasttext_experimental_results[repo], columns=columns)\n",
    "    print(repo)\n",
    "    print(df.groupby([\"Model\"]).mean())\n",
    "    #boxplot = df.boxplot(column=columns) \n",
    "    #plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33464fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_label_data = []\n",
    "for entry in [[\"Heuristic\", heuristic_model_results], [\"IHeuristic\", improved_heuristic_model_results],\n",
    "              [\"BERT\", bert_model_results], [\"FastText\", fasttext_model_results]]:\n",
    "    method = entry[0]\n",
    "    results = entry[1]\n",
    "    for repo in results:\n",
    "        classifier_precision, classifier_recall, classifier_f1, classifier_mcc  = results[repo]\n",
    "        issue_label_data.append([method, repo, classifier_precision, classifier_recall, classifier_f1, classifier_mcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b32080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(issue_label_data, columns=[\"Method\", \"Repo\", \"Precision\", \"Recall\", \"F1\", \"MCC\"])\n",
    "mean_res = df.groupby([\"Repo\", \"Method\"]).mean()\n",
    "print(mean_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696aadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb98ae-bb58-4f09-a21a-425bac968eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_label_data = []\n",
    "for entry in [[\"Heuristic\", heuristics_labels], [\"IHeuristic\", improved_heuristics_labels],\n",
    "              [\"BERT\", bert_labels], [\"FastText\", fasttext_labels]]:\n",
    "    method = entry[0]\n",
    "    labels = entry[1]\n",
    "    for repo in labels:\n",
    "        y = labels[repo][\"Y\"]\n",
    "        p = labels[repo][\"YE\"]\n",
    "        #\n",
    "        tn, fp, fn, tp = confusion_matrix(y, p).ravel()\n",
    "        cnt = tn+fp+fn+tp\n",
    "        file_label_data.append([method, repo, tp, tn, fp, fn, round(100*fp/cnt, 4), round(100*fn/cnt, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45852a5f-6e70-4f2f-a85d-3e1249353a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(file_label_data, columns=[\"Method\", \"Repo\", \"TP\", \"TN\", \"FP\", \"FN\", \"FPR\", \"FNR\"])\n",
    "#mean_res = df.drop(columns=[\"FPR\", \"FNR\"]).groupby([\"Repo\", \"Method\"]).mean()\n",
    "mean_res = df.drop(columns=[\"TP\", \"TN\", \"FP\", \"FN\"]).groupby([\"Method\", \"Repo\"]).mean()\n",
    "print(mean_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_label_data = []\n",
    "for entry in [[\"Heuristic\", heuristics_labels], [\"IHeuristic\", improved_heuristics_labels],\n",
    "              [\"BERT\", bert_labels], [\"FastText\", fasttext_labels]]:\n",
    "    method = entry[0]\n",
    "    labels = entry[1]\n",
    "    for repo in labels:\n",
    "        y = labels[repo][\"Y\"]\n",
    "        p = labels[repo][\"YE\"]\n",
    "        #\n",
    "        classifier_precision = precision_score(y, p)\n",
    "        classifier_recall = recall_score(y, p)    \n",
    "        classifier_f1 = f1_score(y, p)\n",
    "        classifier_mcc = matthews_corrcoef(y, p)\n",
    "        file_label_data.append([method, repo, classifier_precision, classifier_recall, classifier_f1, classifier_mcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(file_label_data, columns=[\"Method\", \"Repo\", \"Precision\", \"Recall\", \"F1\", \"MCC\"])\n",
    "mean_res = df.groupby([\"Repo\", \"Method\"]).mean()\n",
    "print(mean_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88105599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in REPO_TO_ID:\n",
    "    g_df = pd.DataFrame(gold_experimental_results[repo], columns=columns)\n",
    "    h_df = pd.DataFrame(heuristic_experimental_results[repo], columns=columns)\n",
    "    i_df = pd.DataFrame(improved_heuristic_experimental_results[repo], columns=columns)\n",
    "    b_df = pd.DataFrame(bert_experimental_results[repo], columns=columns)\n",
    "    f_df = pd.DataFrame(fasttext_experimental_results[repo], columns=columns)\n",
    "    #\n",
    "    g_df[\"Type\"] = [\"Gold\" for _ in range(len(g_df))]\n",
    "    h_df[\"Type\"] = [\"Heuristic\" for _ in range(len(h_df))]\n",
    "    i_df[\"Type\"] = [\"IHeuristic\" for _ in range(len(i_df))]\n",
    "    b_df[\"Type\"] = [\"BERT\" for _ in range(len(b_df))]\n",
    "    f_df[\"Type\"] = [\"FastText\" for _ in range(len(f_df))]\n",
    "    #\n",
    "    df = pd.concat([g_df, h_df, i_df, b_df, f_df])\n",
    "    #\n",
    "    mean_res = df.groupby([\"Model\", \"Type\"]).mean()\n",
    "    print(repo)\n",
    "    print(mean_res)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea0bbd-9cd7-4a4c-8cb9-474c45894f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics\n",
    "from scipy.stats import normaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc113db-f465-4e63-af62-31803a9c16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_normal(values):\n",
    "    _, p = normaltest(values)\n",
    "    return p >= alpha    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e292ac4-73ef-41a4-884f-18c9d546d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "#\n",
    "cnt = 0\n",
    "normal_cnt = 0\n",
    "for repo in REPO_TO_ID:\n",
    "    g_df = pd.DataFrame(gold_experimental_results[repo], columns=columns)\n",
    "    h_df = pd.DataFrame(heuristic_experimental_results[repo], columns=columns)\n",
    "    i_df = pd.DataFrame(improved_heuristic_experimental_results[repo], columns=columns)\n",
    "    b_df = pd.DataFrame(bert_experimental_results[repo], columns=columns)\n",
    "    f_df = pd.DataFrame(fasttext_experimental_results[repo], columns=columns)\n",
    "    #\n",
    "    g_df[\"Type\"] = [\"Gold\" for _ in range(len(g_df))]\n",
    "    h_df[\"Type\"] = [\"Heuristic\" for _ in range(len(h_df))]\n",
    "    i_df[\"Type\"] = [\"IHeuristic\" for _ in range(len(i_df))]\n",
    "    b_df[\"Type\"] = [\"BERT\" for _ in range(len(b_df))]\n",
    "    f_df[\"Type\"] = [\"FastText\" for _ in range(len(f_df))]\n",
    "    #\n",
    "    df = pd.concat([g_df, h_df, i_df, b_df, f_df])\n",
    "    #\n",
    "    types = set(df[\"Type\"].values)\n",
    "    models = set(df[\"Model\"].values)\n",
    "    models.remove(\"SVM\")\n",
    "    #\n",
    "    for t in types:\n",
    "        for m in models:\n",
    "            cnt = cnt + 1\n",
    "            if is_normal(df[(df[\"Type\"]==t) & (df[\"Model\"]==m)][\"MCC\"].values):\n",
    "                normal_cnt = normal_cnt + 1\n",
    "print(f\"{normal_cnt}/{cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e90b5e-eb60-4423-b993-df43adfb1000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12fed0-cae7-41ac-a466-3af380e4e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "#\n",
    "repo_better = 0\n",
    "repo_important = 0\n",
    "cnt = 0\n",
    "for repo in REPO_TO_ID:\n",
    "    g_df = pd.DataFrame(gold_experimental_results[repo], columns=columns)\n",
    "    h_df = pd.DataFrame(heuristic_experimental_results[repo], columns=columns)\n",
    "    i_df = pd.DataFrame(improved_heuristic_experimental_results[repo], columns=columns)\n",
    "    b_df = pd.DataFrame(bert_experimental_results[repo], columns=columns)\n",
    "    f_df = pd.DataFrame(fasttext_experimental_results[repo], columns=columns)\n",
    "    #\n",
    "    g_df[\"Type\"] = [\"Gold\" for _ in range(len(g_df))]\n",
    "    h_df[\"Type\"] = [\"Heuristic\" for _ in range(len(h_df))]\n",
    "    i_df[\"Type\"] = [\"IHeuristic\" for _ in range(len(i_df))]\n",
    "    b_df[\"Type\"] = [\"BERT\" for _ in range(len(b_df))]\n",
    "    f_df[\"Type\"] = [\"FastText\" for _ in range(len(f_df))]\n",
    "    #\n",
    "    df = pd.concat([g_df, h_df, i_df, b_df, f_df])\n",
    "    #\n",
    "    types = set(df[\"Type\"].values)\n",
    "    models = set(df[\"Model\"].values)\n",
    "    models.remove(\"SVM\")\n",
    "    #\n",
    "    for t in types:\n",
    "        if t == \"BERT\" or t == \"Gold\":\n",
    "            continue\n",
    "        for m in models:\n",
    "            cnt = cnt + 1\n",
    "            \n",
    "            both_normal = is_normal(df[(df[\"Type\"]==\"BERT\") & (df[\"Model\"]==m)][\"MCC\"].values) and is_normal(df[(df[\"Type\"]==t) & (df[\"Model\"]==m)][\"MCC\"].values)\n",
    "            if both_normal:\n",
    "                stat, p = ttest_ind(\n",
    "                        df[(df[\"Type\"]==\"BERT\") & (df[\"Model\"]==m)][\"MCC\"].values, \n",
    "                        df[(df[\"Type\"]==t) & (df[\"Model\"]==m)][\"MCC\"].values\n",
    "                        )\n",
    "            else:\n",
    "                stat, p = mannwhitneyu(\n",
    "                        df[(df[\"Type\"]==\"BERT\") & (df[\"Model\"]==m)][\"MCC\"].values, \n",
    "                        df[(df[\"Type\"]==t) & (df[\"Model\"]==m)][\"MCC\"].values\n",
    "                        )\n",
    "            b_mean = df[(df[\"Type\"]==\"BERT\") & (df[\"Model\"]==m)][\"MCC\"].mean()\n",
    "            o_mean = df[(df[\"Type\"]==t) & (df[\"Model\"]==m)][\"MCC\"].mean()\n",
    "            #\n",
    "            if b_mean > o_mean:\n",
    "                repo_better = repo_better + 1\n",
    "                if p > alpha:\n",
    "                    #print('Same distribution (fail to reject H0)')\n",
    "                    pass\n",
    "                else:\n",
    "                    #print('Different distribution (reject H0)')\n",
    "                    repo_important = repo_important + 1\n",
    "print(f\"{repo_better} {repo_important}/{cnt}\")\n",
    "#65 53/84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89794ffc-9bf4-44d2-a0ad-072bd164b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "#\n",
    "repo_better = 0\n",
    "repo_important = 0\n",
    "cnt = 0\n",
    "for repo in REPO_TO_ID:\n",
    "    g_df = pd.DataFrame(gold_experimental_results[repo], columns=columns)\n",
    "    h_df = pd.DataFrame(heuristic_experimental_results[repo], columns=columns)\n",
    "    i_df = pd.DataFrame(improved_heuristic_experimental_results[repo], columns=columns)\n",
    "    b_df = pd.DataFrame(bert_experimental_results[repo], columns=columns)\n",
    "    f_df = pd.DataFrame(fasttext_experimental_results[repo], columns=columns)\n",
    "    #\n",
    "    g_df[\"Type\"] = [\"Gold\" for _ in range(len(g_df))]\n",
    "    h_df[\"Type\"] = [\"Heuristic\" for _ in range(len(h_df))]\n",
    "    i_df[\"Type\"] = [\"IHeuristic\" for _ in range(len(i_df))]\n",
    "    b_df[\"Type\"] = [\"BERT\" for _ in range(len(b_df))]\n",
    "    f_df[\"Type\"] = [\"FastText\" for _ in range(len(f_df))]\n",
    "    #\n",
    "    df = pd.concat([g_df, h_df, i_df, b_df, f_df])\n",
    "    #\n",
    "    types = set(df[\"Type\"].values)\n",
    "    models = set(df[\"Model\"].values)\n",
    "    models.remove(\"SVM\")\n",
    "    #\n",
    "    for m in models:\n",
    "        cnt = cnt + 1\n",
    "        both_normal = is_normal(df[(df[\"Type\"]==\"BERT\") & (df[\"Model\"]==m)][\"MCC\"].values) and is_normal(df[(df[\"Type\"]==\"Gold\") & (df[\"Model\"]==m)][\"MCC\"].values)\n",
    "        if both_normal:\n",
    "            stat, p = ttest_ind(\n",
    "                    df[(df[\"Type\"]==\"BERT\") & (df[\"Model\"]==m)][\"MCC\"].values, \n",
    "                    df[(df[\"Type\"]==\"Gold\") & (df[\"Model\"]==m)][\"MCC\"].values\n",
    "                    )\n",
    "        else:\n",
    "            stat, p = mannwhitneyu(\n",
    "                    df[(df[\"Type\"]==\"BERT\") & (df[\"Model\"]==m)][\"MCC\"].values, \n",
    "                    df[(df[\"Type\"]==\"Gold\") & (df[\"Model\"]==m)][\"MCC\"].values\n",
    "                    )\n",
    "        #\n",
    "        if p > alpha:\n",
    "            repo_important = repo_important + 1\n",
    "            #print('Same distribution (fail to reject H0)')\n",
    "            pass\n",
    "        else:\n",
    "            #print('Different distribution (reject H0)')\n",
    "            pass\n",
    "print(f\"{repo_important}/{cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a5055-4922-456c-a55b-61694dd58178",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BERT visualize TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8af8cf3-7730-4429-aa65-d15387ab64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in dataset:\n",
    "    print(repo)\n",
    "    #===============================\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f\"./roberta-issue-classifier-{REPO_TO_ID[repo]}/evaluated_model\", num_labels=2)\n",
    "    if device == \"cuda\":\n",
    "        model.cuda()\n",
    "    #\n",
    "    test_nlp = Dataset.from_pandas(pd.DataFrame(nlp_datasets[repo][\"test\"]))\n",
    "    test_nlp = test_nlp.map(tokenize, batched=True)\n",
    "    #\n",
    "    copy_test_nlp = test_nlp.select([i for i in range(len(test_nlp))])\n",
    "    copy_test_nlp = copy_test_nlp.remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n",
    "    copy_test_nlp.set_format(\"torch\")\n",
    "    #\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    #\n",
    "    eval_dataloader = DataLoader(copy_test_nlp, batch_size=16)\n",
    "    #\n",
    "    model.eval()\n",
    "    #\n",
    "    all_last_hiddens = []\n",
    "    all_labels = []\n",
    "    for batch in eval_dataloader:\n",
    "        all_labels.append(batch['labels'].detach().cpu())\n",
    "        del batch[\"issueId\"]\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            hs = model(**batch, output_hidden_states=True)\n",
    "            last_hiddens = hs.hidden_states[-1][:,0,:]\n",
    "            all_last_hiddens.append(last_hiddens)\n",
    "    #\n",
    "    all_labels = torch.cat(all_labels, 0).detach().cpu().numpy()\n",
    "    all_last_hiddens = torch.cat(all_last_hiddens, 0).detach().cpu().numpy()\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(all_last_hiddens)\n",
    "    #\n",
    "    figure(figsize=(8, 6), dpi=80)\n",
    "    plt.scatter(X_embedded[:,0], X_embedded[:,1], c=all_labels, cmap='bwr', s=2)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'tsne-{REPO_TO_ID[repo]}.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdfd35-101b-4ebb-8395-6eb19779e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in dataset:\n",
    "    print(repo)\n",
    "    #======\n",
    "    X = []\n",
    "    Y = []\n",
    "    for file in dataset[repo]:\n",
    "        x = calc_x(dataset[repo][file])\n",
    "        y = calc_y(dataset[repo][file])\n",
    "        #\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(Y)\n",
    "    #======\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "    #\n",
    "    figure(figsize=(8, 6), dpi=80)\n",
    "    plt.scatter(X_embedded[:,0], X_embedded[:,1], c=y, cmap='bwr', s=2)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'file-tsne-{REPO_TO_ID[repo]}.pdf', bbox_inches='tight')\n",
    "    plt.show()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a948fc-3f5f-4c0a-b27c-4fb7dbc6a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in dataset:\n",
    "    print(repo)\n",
    "    #===============================\n",
    "    # Train BERT\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f\"./roberta-issue-classifier-{REPO_TO_ID[repo]}/evaluated_model\", num_labels=2)\n",
    "    if device == \"cuda\":\n",
    "        model.cuda()\n",
    "    #\n",
    "    test_nlp = Dataset.from_pandas(pd.DataFrame(nlp_datasets[repo][\"test\"]))\n",
    "    test_nlp = test_nlp.map(tokenize, batched=True)\n",
    "    #\n",
    "    copy_test_nlp = test_nlp.select([i for i in range(len(test_nlp))])\n",
    "    copy_test_nlp = copy_test_nlp.remove_columns([\"text\"]).rename_column(\"label\", \"labels\")\n",
    "    copy_test_nlp.set_format(\"torch\")\n",
    "    #\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    #\n",
    "    eval_dataloader = DataLoader(copy_test_nlp, batch_size=16)\n",
    "    #\n",
    "    model.eval()\n",
    "    #\n",
    "    all_preds = []\n",
    "    all_issueId = []\n",
    "    for batch in eval_dataloader:\n",
    "        all_issueId.append(batch['issueId'].detach().cpu())\n",
    "        del batch[\"issueId\"]\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            hs = model(**batch, output_hidden_states=True)\n",
    "            logits = hs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            last_hiddens = hs.hidden_states[-1][:,0,:]\n",
    "            all_preds.append(predictions)\n",
    "    #\n",
    "    all_preds = torch.cat(all_preds, 0).detach().cpu().numpy()                          \n",
    "    all_issueId = torch.cat(all_issueId, 0).detach().cpu().numpy()\n",
    "    #\n",
    "    issue_lbl_lookup = {}\n",
    "    for i in range(len(test_nlp)):\n",
    "        issue_bert_lbl = all_preds[i]\n",
    "        issue_id = all_issueId[i]\n",
    "        issue_lbl_lookup[issue_id] = issue_bert_lbl\n",
    "    #===============================\n",
    "    X = []\n",
    "    YH = []\n",
    "    for file in dataset[repo]:\n",
    "        versions = dataset[repo][file]\n",
    "        x = calc_x(versions)\n",
    "        yh = calc_y_bert(versions, issue_lbl_lookup)\n",
    "        #\n",
    "        X.append(x)\n",
    "        YH.append(yh)\n",
    "    X = np.asarray(X)\n",
    "    yh = np.asarray(YH)\n",
    "    #===============================\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "    #\n",
    "    figure(figsize=(8, 6), dpi=80)\n",
    "    plt.scatter(X_embedded[:,0], X_embedded[:,1], c=yh, cmap='bwr', s=2)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'file-bert-tsne-{REPO_TO_ID[repo]}.pdf', bbox_inches='tight')\n",
    "    plt.show()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcff57b-24f4-419e-acab-bd00e13b9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in REPO_TO_ID:\n",
    "    print(repo, REPO_TO_ID[repo])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb5a34-979a-4a0f-a07d-37ac4b96f96b",
   "metadata": {},
   "source": [
    "## Save results to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85952bc6-883c-4601-b060-a8cea92d88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"sdp\": [[\"Gold\", gold_experimental_results], \n",
    "            [\"Heuristic\", heuristic_experimental_results],\n",
    "            [\"IHeuristic\", improved_heuristic_experimental_results],\n",
    "            [\"BERT\", bert_experimental_results],\n",
    "            [\"FastText\", fasttext_experimental_results]],\n",
    "    \"nlp\": [[\"Heuristic\", heuristic_model_results],\n",
    "            [\"IHeuristic\", improved_heuristic_model_results],\n",
    "            [\"BERT\", bert_model_results],\n",
    "            [\"FastText\", fasttext_model_results]],\n",
    "    \"pred\":[[\"Heuristic\", heuristics_labels],\n",
    "            [\"IHeuristic\", improved_heuristics_labels],\n",
    "            [\"BERT\", bert_labels],\n",
    "            [\"FastText\", fasttext_labels]],\n",
    "    \"strategy\":[\n",
    "        [\"IHeuristic\", best_strategy_for_repo]\n",
    "    ]\n",
    "}\n",
    "\n",
    "encoded = jsonpickle.encode(results)\n",
    "with open(os.path.join(DATA_FOLDER, \"issue-article-results-multiple-models_v2.json\"), \"w\") as f_out:\n",
    "    f_out.write(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c54c90-db5d-4866-8dcd-d125d0254c61",
   "metadata": {},
   "source": [
    "## Load results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0419cb-c070-4dc6-a8c8-80679cd82bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_FOLDER, \"issue-article-results-multiple-models_v2.json\"), \"r\") as f_in:\n",
    "    for line in f_in:\n",
    "        results = jsonpickle.decode(line)\n",
    "\n",
    "if \"sdp\" in results:\n",
    "    for entry in results[\"sdp\"]:\n",
    "        if entry[0] == \"Gold\":\n",
    "            gold_experimental_results = entry[1]\n",
    "        if entry[0] == \"Heuristic\":\n",
    "            heuristic_experimental_results = entry[1]\n",
    "        if entry[0] == \"IHeuristic\":\n",
    "            improved_heuristic_experimental_results = entry[1]\n",
    "        if entry[0] == \"BERT\":\n",
    "            bert_experimental_results = entry[1]\n",
    "        if entry[0] == \"FastText\":\n",
    "            fasttext_experimental_results = entry[1]\n",
    "\n",
    "if \"nlp\" in results:\n",
    "    for entry in results[\"nlp\"]:\n",
    "        if entry[0] == \"BERT\":\n",
    "            bert_model_results = entry[1]\n",
    "        if entry[0] == \"FastText\":\n",
    "            fasttext_model_results = entry[1]\n",
    "        if entry[0] == \"Heuristic\":\n",
    "            heuristic_model_results = entry[1]\n",
    "        if entry[0] == \"IHeuristic\":\n",
    "            improved_heuristic_model_results = entry[1]\n",
    "\n",
    "if \"pred\" in results:\n",
    "    for entry in results[\"pred\"]:\n",
    "        if entry[0] == \"BERT\":\n",
    "            bert_labels = entry[1]\n",
    "        if entry[0] == \"FastText\":\n",
    "            fasttext_labels = entry[1]\n",
    "        if entry[0] == \"Heuristic\":\n",
    "            heuristics_labels = entry[1]\n",
    "        if entry[0] == \"IHeuristic\":\n",
    "            improved_heuristics_labels = entry[1]\n",
    "\n",
    "if \"strategy\" in results:\n",
    "    for entry in results[\"strategy\"]:\n",
    "        if entry[0] == \"IHeuristic\":\n",
    "            best_strategy_for_repo = entry[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f423ca1b-61ff-4482-89e8-32a99e4b2aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
